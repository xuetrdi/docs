* Attention
  
** What is Attention in Deep Learning
   注意力机制本质上是对输入特征的贡献进行不均匀的加权，从而优化学习某个目标的过程的方法。
   注意表示选择过程，其中某些输入比其他输入尤为重要。
   
   无论如何构建，神经网络都会自学将输入数据中某些部分的连接放在较高的权重上，而将其他部分的连接放在较小的权重上。

** 注意力的方法
   - 对给定级别的所有输入进行线性变换。输入转换后称为keys，然后我们希望解释的当前输入也被称为查询(query),
     线性变换进入一个空间，一边keys和query之间的相似性在给定query的转换过程中携带有关这些keys的相关性信息。
     我们确定key和query之间的相似性，并将该相似性用作值的线性凸组合的权重。
     这些值是查询级别的功能(包括自身)。求和称为上下文向量(context vector)，并馈入模型下一层的相应单元中
