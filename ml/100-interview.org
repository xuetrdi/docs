* 机器学习算法工程师的自我修养
  
** 第一章 特征工程
*** 特征归一化
    为什么需要对数值类型的特征做归一化?
    - 为了消除数据特征之间的量纲影响,我们需要对特征进行归一化处理,使得不同指标之间具有可比性
    最常用的两种归一化方法:
    - 线性函数归一化: 对原始数据进行线性变换,使结果映射到[0,1]的范围,实现对原始数据的等比缩放。
    - 零均归一化: 将原始数据映射到均值为 0,标准差为 1 的分布上。
    
    线性函数归一化:
               $X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$
    零均值归一化:
               $z=\frac{x-\mu}{\sigma}$
    
*** 类别型特征
    类别型特征(Categoridal Feature)主要是指性别，血型等在有限选项内取值的特征。
    类别型特征原始输入通常是字符串形式，除了决策树等少量模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，必须转换成数值类型才能正确工作
    
    在数据进行预处理时，应该怎样处理类别型特征？
    - 序列编码(Ordinal Encoding):通常用于处理类别间具有大小关系的数据。比如成绩
    - 独热编码(One-hot Encoding):通常用于处理类别间不具有大小关系的特征。
    - 二进制编码(Binary Encoding):二进制编码主要分两步，先用序号编码给每个类别赋予一个类别 ID,然后将类别 ID 对应的二进制编码作为结果。
*** 高维组合特征
    + 什么是组合特征?
      - 为了提高复杂关系的拟合能力,在特征工程中经常会把一阶离散特征两两组合,构成高阶组合特征。
    + 如何处理高维组合特征?
      - 把两个特征组成一个特征对
*** 组合特征
    + 怎样有效地找到组合特征?
*** 文本表示模型
    - 词袋模型(Bag of Words)
    - TF-IDF(Term Frequency-Inverse Document Frequency)
    - 主题模型(Topic Model)
    - 词嵌入模型(Word Embedding)
    文本表示模型以及优缺点:
    + 词袋模型
      - 最基础的文本表示模型是词袋模型:
      - 将每篇文章看成一袋子词,并忽略每个词出现的顺序
      - 以词切分,每篇文章表示成一个长向量,向量中的每一维代表一个词。
      - 该维度对应的权重反映了这个词在原文中的重要程度,常用 TF-IDF 计算权重: TF-IDF(t,d)=TF(t,d)xIDF(t),其中 TF(t,d)为单词在 t 在文档 d 中出现的频率,IDF(t)是逆文档频率。
        IDF(t)=log(文章总数/包含单词 t 的文章总数+1)
    + N-gram 模型
      - 连续出现的 n 个词组成的词组(N-gram)也作为一个单独的特征放到向量表示中去,构成 N-gram 模型。
      - 同一个词可能有多种词性变化,却具有相似含义。
      - 实际应用中一般会对单词进行词干抽取(Word Stemming)处理,即将不同词性的单词统一成为同一词干的形式。
    + 主题模型
      - 用于从文本库中发现有代表性的主题(得到每个主题上面词的分布特征),并且能够计算每篇文章的主题分布。
    + 词嵌入与深度学习模型
      - 词嵌入是一类将词向量化的模型的统称,核心思想是将每个词都映射成低纬空间 K 维(5-300)上一个稠密向量(Dense Vector),K 维空间的每一个维可以看作一个隐含的主题,只不过不像主题模型中那么直观。
*** Word2Vec
    谷歌 2013 年提出的 Word2Vec 是目前最常用的词嵌入模型之一。
    Word2Vec 实际是一种浅层的神经网络模型,它有两种网络结构,分别是 CBOW(Continues Bag of Words)和 Skip-gram。
    + Word2Vec 是如何工作的?
      - CBOW 的目标是根据上下文出现的词语来预测当前词的生成概率
      - Skip-gram 是根据当前词来预测上下文中各词的生成概率
      - CBOW 和 Skip-gram 都可以表示成由输入层(Input),映射层(Projection)和输出层(Output)组成的神经网络
    
    其中 w(t)是当前所关注的词,w(t-2),w(t-1),w(t+1),w(t+2)是上下文出现的词,这里的前后滑动窗口大小均设为 2。
    + CBOW 和 Skip-gram 的模型结构:
      - 输入层: 每个词由独热编码表示,即所有词均表示成一个 N 维向量,其中 N 为词汇表中单词的总数。
      - 映射层: K 个隐含单元的取值可以由 N 维输入向量以及连接输入和隐含单元之间 NxK 维权重矩阵计算得到,在 CBOW 中,还需要将各个输入词所计算出的隐含单元求和。
      - 输出层: 输出层向量的值通过隐含层向量(K 维),以及连接隐含层和输出层之间的 KxN 维权重矩阵计算得到。输出层也是 N 维向量。
      - Softmax: 计算每个词的生成概率。
    + Word2Vec 与 LDA(因狄利克雷模型)的区别和联系
      - LDA 是利用文档中单词的共现关系来对单词按主题聚类,也可以理解为对"文档-单词"矩阵进行分解,得到"文档-主题"和"主题-单词"两个概率分布。
      - Word2Vec 其实是对"上下文-单词"矩阵进行学习,其中上下文由周围的几个单词组成,由此得到的词向量表示更多地融入了上下文共现的特征.
      - 如果两个单词所对应的 Word2Vec 向量相似度较高,那么它们很可能经常在同样的上下文出现。
      - 主题模型和词嵌入两类的主要差异:主题模型通过一定的结构调整可以基于"上下文-单词"矩阵进行主题推理;词嵌入根据"上下文-单词"矩阵学习出现的隐含向量表示。
      - 主题模型和词嵌入两类方法最大的不同在于模型本身,主题模型是一种基于概率图模型的生成式模型,其似然函数可以写成若干条件概率连成的形式,其中也需要推测的隐含变量(即主题)
        而词嵌入模型一般表达为神经网络形式,似然函数定义在网络的输出上,需要通过学习网络的权重以得到单词的稠密向量表示。
*** 图像不足时的处理方法
    机器学习应用中经常会遇到训练数据不足的问题。
    处理方法:
    - 迁移学习
    - 生成对抗网络
    - 图像处理
    - 上采样技术
    - 数据扩充
   一个模型所能提供的信息一般来源于两个方面: 
   - 训练数据中蕴含的信息
   - 在模型的形成过程中(构造,学习,推理),人们提供的先验信息
   当数据不足时,说明模型从原始数据中获取的信息比较少,这种情况下想要保证模型的效果,就需要更多先验信息。
   
   先验信息可以作用在模型上
   - 让模型采用特定的内在结构、条件假设或添加其它一些约束条件
   - 先验信息也可以直接施加在数据集上,即根据特定的先验假设去调整、变换、扩展训练数据,让其展现出更多更有用的信息,以利于后续模型的训练和学习。
   
   在图像分类任务上,训练数据不足带来的问题主要表现在**过拟合**方面,在训练集上效果很好,在测试集上表现很差。
   处理过拟合的方法主要两类:
   - 基于模型的方法: 简化模型(非线性简化为线性),添加约束项(L1/L2 正则),集成学习,Dropout
   - 基于数据的方法: 对图像进行扩充增强,随机旋转,平移,缩放,裁剪,填充,翻转;添加噪声(椒盐噪声,高斯白噪声);颜色变换;亮度,清晰度,对比度,锐度;GAN
   借助已有的其它模型或数据来进行迁移学习:
   - 在大规模数据集上预训练好的通用模型,在针对目标任务的小数据集上进行微调,看作一种简单的迁移学习。
   
** 第二章 模型评估
*** 评估指标的局限性
    没有测量,就没有科学。
    模型评估主要分为离线评估和在线评估两个阶段。
    针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。
    针对性选择合适的评估指标、根据评估指标的反馈进行模型调整。
    + 各种指标
      - 准确率: Accuracy
      - 精确率: Precision
      - 召回率: Recall
      - 均方根误差: Root Mean Square Error(RMSE)
    
    + 准确率的局限性
      - Accuracy = 正确分类的样本量/总样本量
      - 缺陷性: 当负样本占 99%,分类负样本也可以获得 99%的准确率。所以不均衡是准确率影响的最主要因素。
      - 解决方法: 可以使用更为有效的平均准确率(每个类别下的样本准确率的算术平均)作为模型评估的指标。
    
    + 精确率与召回率的权衡
      - 精确率是指分类正确的正样本个数占分类器判定为正样本的样本个数比例: TP/(TP+FP+FN)
      - 召回率是指分类正确的正样本占真正的正样本个数的比例: TP/(TP+FP)
      - P-R 曲线能更好的衡量模型的性能
    
    + F1 score 和 ROC 曲线也能综合地反映一个排序模型的性能。
      - F1 score 是精准率和召回率的调和平均值。

    + RMSE 经常被用来衡量回归模型的好坏
      - RMSE 能够很好地反映地反映回归模型预测值与真实值的偏离程度。
      - 如果存在个别偏离程度非常大的离群点时,即使数量非常少,也会让 RMSE 指标变得很差。
      - 存在比 RMSE 的鲁棒性更好的指标,比如绝对百分比误差(Mean Absolute Percent Error，MAPE)
*** ROC 曲线
    ROC 曲线：Receiver Operating Characteristic Curve(受试者工作特征曲线),
    + ROC 曲线
      - 横坐标为假阳性率(False Positive Rate,FPR): FPR = FP/N, N 是真实的负样本的数量(N=TN+FN)
      - 纵坐标为真阳性率(True Positive Rate, TPR): TPR = TP/P, P 是真实的正样本的数量(P=TP+FP)
    + 绘制 ROC 曲线
      - 统计正样本和负样本的数量 P 和 N
      - 把横轴的刻度间隔设置为 1/N,纵轴的刻度间隔设置为 1/P
      - 根据模型输出的预测概率对样本进行排序(从高到低)
      - 依次遍历样本,同时从零开始绘制 ROC 曲线,没遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线,每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线,直到遍历所有样本
      - 曲线最终停在(1,1)这个点,整个 ROC 曲线绘制完成。
    + 如何计算 AUC
      - AUC 指的是 ROC 曲线下的面积大小
      - 该值能够量化地反映基于 ROC 曲线衡量出的模型性能
      - 计算 AUC 值只需要沿着 ROC 横轴做积分就可以了
      - ROC 曲线一般都处于 y=x 这条直线的上方,如果不是,只要把模型预测概率反转成 1-p 就可以得到一个更好的分类器
      - AUC 的取值一般在 0.5-1 之间,AUC 越大,说明分类器越可能把真正的正样本排在前面,分类性能越好
    + ROC 曲线和 P-R 曲线的特点
      - P-R 曲线经常被用来做分类和排序模型,当正负样本发生变化时,一般会发生剧烈的变化
      - ROC 曲线有一个特点,当正负样本的分布发生变化时,ROC 曲线的形状能够基本保持不变,让 ROC 曲线能够降低不同测试集带来的干扰,更加客观地衡量模型本身的性能。
      - 在实际问题数量往往很不均衡,P-R 曲线的变化就会非常大,ROC 曲线则更加稳定地反映模型本身的好坏。
*** 余弦距离的应用
*** A/B 测试的陷阱
*** 模型评估的方法
*** 超参数调优
*** 过拟合与欠拟合
** 第三章 经典算法
   
** 第四章 降维

** 第五章 非监督学习

** 第六章 概率图模型
   
** 第七章 优化算法

** 第八章 采样

** 第九章 前向神经网络
   
** 第十章 循环神经网络

** 第十一章 强化学习

** 第十二章 继承学习

** 第十三章 生成对抗网络
