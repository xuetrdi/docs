* 机器学习算法工程师的自我修养
  
** 第一章 特征工程
*** 特征归一化
    为什么需要对数值类型的特征做归一化?
    - 为了消除数据特征之间的量纲影响,我们需要对特征进行归一化处理,使得不同指标之间具有可比性
    最常用的两种归一化方法:
    - 线性函数归一化: 对原始数据进行线性变换,使结果映射到[0,1]的范围,实现对原始数据的等比缩放。
    - 零均归一化: 将原始数据映射到均值为 0,标准差为 1 的分布上。
    
    线性函数归一化:
               $X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$
    零均值归一化:
               $z=\frac{x-\mu}{\sigma}$
    
*** 类别型特征
    类别型特征(Categoridal Feature)主要是指性别，血型等在有限选项内取值的特征。
    类别型特征原始输入通常是字符串形式，除了决策树等少量模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，必须转换成数值类型才能正确工作
    
    在数据进行预处理时，应该怎样处理类别型特征？
    - 序列编码(Ordinal Encoding):通常用于处理类别间具有大小关系的数据。比如成绩
    - 独热编码(One-hot Encoding):通常用于处理类别间不具有大小关系的特征。
    - 二进制编码(Binary Encoding):二进制编码主要分两步，先用序号编码给每个类别赋予一个类别 ID,然后将类别 ID 对应的二进制编码作为结果。
*** 高维组合特征
    + 什么是组合特征?
      - 为了提高复杂关系的拟合能力,在特征工程中经常会把一阶离散特征两两组合,构成高阶组合特征。
    + 如何处理高维组合特征?
      - 把两个特征组成一个特征对
*** 组合特征
    + 怎样有效地找到组合特征?
*** 文本表示模型
    - 词袋模型(Bag of Words)
    - TF-IDF(Term Frequency-Inverse Document Frequency)
    - 主题模型(Topic Model)
    - 词嵌入模型(Word Embedding)
    文本表示模型以及优缺点:
    + 词袋模型
      - 最基础的文本表示模型是词袋模型:
      - 将每篇文章看成一袋子词,并忽略每个词出现的顺序
      - 以词切分,每篇文章表示成一个长向量,向量中的每一维代表一个词。
      - 该维度对应的权重反映了这个词在原文中的重要程度,常用 TF-IDF 计算权重: TF-IDF(t,d)=TF(t,d)xIDF(t),其中 TF(t,d)为单词在 t 在文档 d 中出现的频率,IDF(t)是逆文档频率。
        IDF(t)=log(文章总数/包含单词 t 的文章总数+1)
    + N-gram 模型
      - 连续出现的 n 个词组成的词组(N-gram)也作为一个单独的特征放到向量表示中去,构成 N-gram 模型。
      - 同一个词可能有多种词性变化,却具有相似含义。
      - 实际应用中一般会对单词进行词干抽取(Word Stemming)处理,即将不同词性的单词统一成为同一词干的形式。
    + 主题模型
      - 用于从文本库中发现有代表性的主题(得到每个主题上面词的分布特征),并且能够计算每篇文章的主题分布。
    + 词嵌入与深度学习模型
      - 词嵌入是一类将词向量化的模型的统称,核心思想是将每个词都映射成低纬空间 K 维(5-300)上一个稠密向量(Dense Vector),K 维空间的每一个维可以看作一个隐含的主题,只不过不像主题模型中那么直观。
*** Word2Vec
    谷歌 2013 年提出的 Word2Vec 是目前最常用的词嵌入模型之一。
    Word2Vec 实际是一种浅层的神经网络模型,它有两种网络结构,分别是 CBOW(Continues Bag of Words)和 Skip-gram。
    + Word2Vec 是如何工作的?
      - CBOW 的目标是根据上下文出现的词语来预测当前词的生成概率
      - Skip-gram 是根据当前词来预测上下文中各词的生成概率
      - CBOW 和 Skip-gram 都可以表示成由输入层(Input),映射层(Projection)和输出层(Output)组成的神经网络
    
    其中 w(t)是当前所关注的词,w(t-2),w(t-1),w(t+1),w(t+2)是上下文出现的词,这里的前后滑动窗口大小均设为 2。
    + CBOW 和 Skip-gram 的模型结构:
      - 输入层: 每个词由独热编码表示,即所有词均表示成一个 N 维向量,其中 N 为词汇表中单词的总数。
      - 映射层: K 个隐含单元的取值可以由 N 维输入向量以及连接输入和隐含单元之间 NxK 维权重矩阵计算得到,在 CBOW 中,还需要将各个输入词所计算出的隐含单元求和。
      - 输出层: 输出层向量的值通过隐含层向量(K 维),以及连接隐含层和输出层之间的 KxN 维权重矩阵计算得到。输出层也是 N 维向量。
      - Softmax: 计算每个词的生成概率。
    + Word2Vec 与 LDA(因狄利克雷模型)的区别和联系
      - LDA 是利用文档中单词的共现关系来对单词按主题聚类,也可以理解为对"文档-单词"矩阵进行分解,得到"文档-主题"和"主题-单词"两个概率分布。
      - Word2Vec 其实是对"上下文-单词"矩阵进行学习,其中上下文由周围的几个单词组成,由此得到的词向量表示更多地融入了上下文共现的特征.
      - 如果两个单词所对应的 Word2Vec 向量相似度较高,那么它们很可能经常在同样的上下文出现。
      - 主题模型和词嵌入两类的主要差异:主题模型通过一定的结构调整可以基于"上下文-单词"矩阵进行主题推理;词嵌入根据"上下文-单词"矩阵学习出现的隐含向量表示。
      - 主题模型和词嵌入两类方法最大的不同在于模型本身,主题模型是一种基于概率图模型的生成式模型,其似然函数可以写成若干条件概率连成的形式,其中也需要推测的隐含变量(即主题)
        而词嵌入模型一般表达为神经网络形式,似然函数定义在网络的输出上,需要通过学习网络的权重以得到单词的稠密向量表示。
*** 图像不足时的处理方法
    机器学习应用中经常会遇到训练数据不足的问题。
    处理方法:
    - 迁移学习
    - 生成对抗网络
    - 图像处理
    - 上采样技术
    - 数据扩充
   一个模型所能提供的信息一般来源于两个方面: 
   - 训练数据中蕴含的信息
   - 在模型的形成过程中(构造,学习,推理),人们提供的先验信息
   当数据不足时,说明模型从原始数据中获取的信息比较少,这种情况下想要保证模型的效果,就需要更多先验信息。
   
   先验信息可以作用在模型上
   - 让模型采用特定的内在结构、条件假设或添加其它一些约束条件
   - 先验信息也可以直接施加在数据集上,即根据特定的先验假设去调整、变换、扩展训练数据,让其展现出更多更有用的信息,以利于后续模型的训练和学习。
   
   在图像分类任务上,训练数据不足带来的问题主要表现在**过拟合**方面,在训练集上效果很好,在测试集上表现很差。
   处理过拟合的方法主要两类:
   - 基于模型的方法: 简化模型(非线性简化为线性),添加约束项(L1/L2 正则),集成学习,Dropout
   - 基于数据的方法: 对图像进行扩充增强,随机旋转,平移,缩放,裁剪,填充,翻转;添加噪声(椒盐噪声,高斯白噪声);颜色变换;亮度,清晰度,对比度,锐度;GAN
   借助已有的其它模型或数据来进行迁移学习:
   - 在大规模数据集上预训练好的通用模型,在针对目标任务的小数据集上进行微调,看作一种简单的迁移学习。
   
** 第二章 模型评估
*** 评估指标的局限性
    没有测量,就没有科学。
    模型评估主要分为离线评估和在线评估两个阶段。
    针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。
    针对性选择合适的评估指标、根据评估指标的反馈进行模型调整。
    + 各种指标
      - 准确率: Accuracy
      - 精确率: Precision
      - 召回率: Recall
      - 均方根误差: Root Mean Square Error(RMSE)
    
    + 准确率的局限性
      - Accuracy = 正确分类的样本量/总样本量
      - 缺陷性: 当负样本占 99%,分类负样本也可以获得 99%的准确率。所以不均衡是准确率影响的最主要因素。
      - 解决方法: 可以使用更为有效的平均准确率(每个类别下的样本准确率的算术平均)作为模型评估的指标。
    
    + 精确率与召回率的权衡
      - 精确率是指分类正确的正样本个数占分类器判定为正样本的样本个数比例: TP/(TP+FP+FN)
      - 召回率是指分类正确的正样本占真正的正样本个数的比例: TP/(TP+FP)
      - P-R 曲线能更好的衡量模型的性能
    
    + F1 score 和 ROC 曲线也能综合地反映一个排序模型的性能。
      - F1 score 是精准率和召回率的调和平均值。

    + RMSE 经常被用来衡量回归模型的好坏
      - RMSE 能够很好地反映地反映回归模型预测值与真实值的偏离程度。
      - 如果存在个别偏离程度非常大的离群点时,即使数量非常少,也会让 RMSE 指标变得很差。
      - 存在比 RMSE 的鲁棒性更好的指标,比如绝对百分比误差(Mean Absolute Percent Error，MAPE)
*** ROC 曲线
    ROC 曲线：Receiver Operating Characteristic Curve(受试者工作特征曲线),
    + ROC 曲线
      - 横坐标为假阳性率(False Positive Rate,FPR): FPR = FP/N, N 是真实的负样本的数量(N=TN+FN)
      - 纵坐标为真阳性率(True Positive Rate, TPR): TPR = TP/P, P 是真实的正样本的数量(P=TP+FP)
    + 绘制 ROC 曲线
      - 统计正样本和负样本的数量 P 和 N
      - 把横轴的刻度间隔设置为 1/N,纵轴的刻度间隔设置为 1/P
      - 根据模型输出的预测概率对样本进行排序(从高到低)
      - 依次遍历样本,同时从零开始绘制 ROC 曲线,没遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线,每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线,直到遍历所有样本
      - 曲线最终停在(1,1)这个点,整个 ROC 曲线绘制完成。
    + 如何计算 AUC
      - AUC 指的是 ROC 曲线下的面积大小
      - 该值能够量化地反映基于 ROC 曲线衡量出的模型性能
      - 计算 AUC 值只需要沿着 ROC 横轴做积分就可以了
      - ROC 曲线一般都处于 y=x 这条直线的上方,如果不是,只要把模型预测概率反转成 1-p 就可以得到一个更好的分类器
      - AUC 的取值一般在 0.5-1 之间,AUC 越大,说明分类器越可能把真正的正样本排在前面,分类性能越好
    + ROC 曲线和 P-R 曲线的特点
      - P-R 曲线经常被用来做分类和排序模型,当正负样本发生变化时,一般会发生剧烈的变化
      - ROC 曲线有一个特点,当正负样本的分布发生变化时,ROC 曲线的形状能够基本保持不变,让 ROC 曲线能够降低不同测试集带来的干扰,更加客观地衡量模型本身的性能。
      - 在实际问题数量往往很不均衡,P-R 曲线的变化就会非常大,ROC 曲线则更加稳定地反映模型本身的好坏。
*** 余弦距离的应用
    评估样本间距离
    在机器学习中特征表示成向量,分析两个特征向量之间的相似性常用余弦相似度来表示。
    $$\cos(A,B)=\frac{A\cdot B}{||A||_{2}||B||_{2}}$$  : 即两个向量夹角的余弦,关注的是向量之间的角度关系,并不关心它们的绝对值的大小,其取值范围是[-1,1]
    在 Word2Vec 中,其向量的模长是经过归一化的,此时欧式距离与余弦距离有着单调的关系
    $$||A-B||_{2}=\sqrt{2(1-\cos(A,B))}$$
    其中欧氏距离: $||A-B||_{2}$
    其中余弦相似度: $\cos(A,B)$
    表示余弦距离: $(1-cos(A,B))$
    总体来说,欧式距离体现**数值**上的绝对差异,而余弦**距离**体现方向上的相对差异。
    
    三条距离公理:正定性、对称性、三角不等式成立,则该实数可称为这对元素之间的距离。
    余弦距离满足正定性和对称性,但不满足三角不等式,因此它并不是严格定义的距离。
    
*** A/B 测试的陷阱
    在离线评估后,为什么还要进行 A/B 测试
    - 离线评估无法完全消除模型过拟合的影响,因此得出的离线评估结果无法完全替代线上评估结果。
    - 离线评估无法完全还原线上的环境。
    - 线上系统的某些商业指标在离线评估中无法计算。离线测试只针对模型本身进行评估,与模型无关的其它指标,特别是商业指标。
    A/B 测试的主要手段是进行用户分桶,即将用户分成实验组和对照组,对实验组的用户使用新模型,对对照组的用户使用旧模型。
    分桶过程中要注意:无偏性,和随机性。
*** 模型评估的方法
    模型验证中主要的方法：
    - Holdout 检验: 简单、直接,将样本按照比例划分,计算 ROC 曲线,精确率,召回率等指标评估模型性能。缺点:分组不平衡。
    - k-fold 交叉检验: 全部样本划分成 k 个大小相等的样本子集。依次把当前子集作为验证集,其余所有子集作为训练集,进行模型的训练和评估。最后把 k 次评估指标的平均值作为最终的评估指标。每次留下一个作为验证集,其余作为测试集。
    - 自助法: 不管是 Holdout 检验还是交叉检验,都是基于划分训练集和测试集的方法进行模型评估的。基于自助采样法的检验方法。
*** 超参数调优
    超参数调优方法:
    - 网格搜索: 通过查找搜索范围内的所有的点来确定最优值,通过网格搜索很大概率能找到全局最优值,目标函数一般非凸,所以可能错过全局最优值。
    - 随机搜索: 随机搜索的思想与网格搜索比较相似,只是不再测试上界和下界之间的所有值,而是随机范围中随机选取样本点。
    - 贝叶斯优化: 贝叶斯优化算法是通过目标函数形状进行学习,找到使目标函数向全局最优值提升的参数。一旦找到局部最优解,会在该区域不断采样,所以很容易陷入局部最优值。
    超参数搜索算法一般包含以下因素
    - 目标函数
    - 搜索范围
    - 其它参数,搜索步长
*** 过拟合与欠拟合
    过拟合: 在训练集上拟合很好,在测试集上表现很差。
    欠拟合: 没有很好地捕捉到数据的特征,不能够很好地拟合数据。
    降低过拟合风险的方法:
    - 增加训练数据
    - 降低模型复杂度
    - 正则化
    - 集成学习方法,把多个模型集成在一起,降低单一模型的过拟合风险
    降低欠拟合风险的方法:
    - 添加新特征
    - 增加模型复杂度
    - 减小正则化系数:正则化是用来防止过拟合,但当模型出现欠拟合想象,则需要有针对地减小正则化系数
** 第三章 经典算法
*** SVM 模型推导
    任意线性可分的两组点,它们在 SVM 分类超平面上的投影都是线性不可分的。
    + KKT 条件
    + SVM 的分类结果仅依赖于支持向量,也就是 SVM 有用极高运行效率的关键之一
    + 凸优化理论中的超平面分离定理(SHT)
      - 对于不相交的两个凸集,存在一个超平面,将两个凸集分离。
      - 对于二维的情况,两个凸集间距离最短两点连线的中垂线就是一个将它们分离的超平面
    借助这个定义,可以先对线性可分的两组点各自求各自的凸包,不难发现,SVM 求得的超平面就是两个凸包上距离最短的两点连线的中垂线。
    根据凸包的性质很容易知道:
    - 凸包上的点要么是样本点,要么处于两个样本点的连线上
    两个凸包间距离最短的两个点可以分为三种情况：
    - 两边的点均为样本点
    - 一边的点为样本点,另一边的点在样本点的连线上
    - 两边的点均在连线上
    核函数
    SMO(Sequential Minimal Optimization)
*** 逻辑回归   
    逻辑回归处理的是分类问题,因变量取值是一个二元分布,模型得出的是期望: $E\left[y|x;\theta \right]$
    线性回归处理的是回归问题,线性回归求解的问题是: $y=\theta^{T}x+\epsilon$, $\epsilon$ 代表误差项
    逻辑回归的因变量是离散的,线性回归中的因变量是连续的,逻辑回归可以看做广义线性模型,在因变量 y 服从二元分布时的一个特殊情况，
       而使用最小二乘法求解线性回归时,我们认为因变量 y 服从正太分布
    逻辑回归和线性回归二者都使用了极大似然估计来对训练样本进行建模
    多分类问题的概率服从于几何分布,使用多项逻辑回归(Softmax Regression)来进行分类
    多项逻辑回归实际上是二分类逻辑回归在多标签分类下的一种拓展;当存在样本可能属于多个标签时,可以训练 k 个二分类的逻辑回归分类器
*** 决策树
    决策树是一种自上而下,对样本数据进行树形分类的过程,由节点和有向边组成
    决策树作为最基础、最常见的有监督学习模型,常被用于分类问题和回归问题
    决策树的三个处理过程:
    - 特征选择
    - 树的构造
    - 树的剪枝
    根据不同的特征和属性,建立一颗树形的分类结构。
    若干不同的决策树中选取最优的决策树是一个 NP 完全问题,通常采用启发式学习的方法构建一颗决策树。
    决策树的常用算法:
    - ID3: 最大信息增益
    - C4.5: 最大信息增益比
    - CART: 最大基尼指数(Gini),Gini 是描述的数据纯度
    决策树的剪枝方法(防止过拟合):
    - 预剪枝
    - 后剪枝
** 第四章 降维
   为什么要降维?
   - 对原始数据进行特征提取,有时会得到比较高的特征向量,计算的速度特别慢
   - 通过降维来寻找数据内部的特征,从而提升表达能力,降低训练复杂度
   常见的降维的方法有:
   - 主成分分析
   - 线性判别分析
   - 等距映射
   - 局部线性嵌入
   - 拉普拉斯特征映射
   - 局部保留映射
   PCA:
   - 线性,**非监督**,全局降维算法
   - 旨在找到数据的主成分,并利用这些成分表征原始数据,从而达到降维的目的
   - 通过坐标系旋转使得新坐标系能表征原生数据,完成数据的降维
   - 使得原生数据更分散,则信号具有较大方差,噪声具有较小方差(信噪比)
   PCA 的目标:
   - 信噪比越小意味着数据的质量越差,即最大化投影方差,也就是让数据在主轴投影的方差最大
   - 投影后的方差就是协方差矩阵的特征值
   - 最大的方差也就是协方差矩阵最大的特征值
   - 最佳投影方向就是最大特征值所对应的特征向量
   - 次佳投影方向就是最佳投影方向的正交空间中,是第二大特征值所对应的特征向量
   PCA 求解方法:
   - 对样本数据进行中心化处理
   - 求样本协方差矩阵
   - 对协方差矩阵进行特征值分解,将特征值从大到小排列
   - 取特征值前 d 大对应的特征向量,通过以下映射将 n 维样本映射到 d 维度
   PCA 的不足:
   - PCA 是一种线性降维方法,有一定局限性
   - 可以通过核映射对 PCA 进行扩展得到核主成分(KPCA)
   非线性降维(流形映射):
   - 等距降维
   - 局部线性嵌入
   - 拉普拉斯特征映射
  
   线性判别分析:
   - **有监督**学习算法
   - 经常被用来对数据进行降维
   中心思想:
   - 最大化类间距离和最小化类内距离
   - 使得类间距离尽可能大的投影方式,现在只需要同时优化类内方差,使其尽可能小。
   - LDA 已被证明是非常有效的一种降维方法
   - 线性模型对于噪声的鲁棒性比较好
   - 由于模型简单,表达能力有一定局限性,通过引入核函数扩展 LDA 方法以处理分布较为复杂的数据
   
   多个类别标签高维数据的 LDA 求解方法
   1. 计算数据集中每个类别样本的均值向量($\mu_{j}$)和总体均值向量($\mu$)
   2. 类内散度矩阵($S_{w}$),全局散度矩阵($S_{t}$),得到类间散度矩阵($S_{b}=S_{t}-S_{w}$)
   3. 对矩阵($S_{w}^{-1}S_{b}$进行特征值分解,将特征值从大到小排列
   4. 取特征值前 d 大对应的特征向量,则将 n 维降到 d 维
   PCA 与 LDA 的区别:
   - PCA 选择的是投影后数据方差最大的方向,主成分表示原始数据可以去除冗余的维度,达到降维
   - LDA 选择的是投影后类内方差小、类间方差大的方向
   特征脸(Eigenface):
   - 基于 PCA 的人脸识别方法也称为特征脸方法,该方法将人脸图像按行展开形成一个高维向量,对多个人脸特征的协方差矩阵做特征值分解,其中较大特征值对应的特征向量具有与人脸相似的形状,故称为特征脸
   基本准则:
   - 对无监督的任务使用 PCA 进行降维
   - 对有监督的则应用 LDA
   - PCA 和 LDA 这种经典的线性降维方法,对于非线性数据,可以通过核映射等方法对二者分别进行扩展以得到更好的降维效果。
** 第五章 非监督学习
*** K 均值聚类
    分类问题属于监督学习,聚类则是非监督学习。
    K 均值聚类(K-Means Clustering)是最基础和最常用的聚类算法。
    基本思想:通过迭代方式寻找 K 个簇(Cluster)的一种划分方案,使得聚类结果对应的代价函数最小。
    代价函数可以定义为各个样本距离所属簇中心点的误差平方和:
    $J(c,\mu)=\sum_{i=1}^{M}\|x_{i}-\mu_{c_{i}}\|^2$ , $x_{i}$代表第 i 个样本,$c_{i}$是$x_{i}$所属的簇,$\mu_{c_{i}}$对应的中心点,M 是样本总量
    
    EM(Expectation-Maximization:最大期望)算法
    
    K 均值算法的具体步骤:
    1. 数据预处理,如归一化,离群点处理
    2. 随机选取 K 个簇中心,记为$\mu_{1}\,\mu_{2}\,...\mu_{K}$
    3. 定义代价函数:
    4. 令 t=0,1,2,...为迭代步数,重复下面过程直到 J 收敛
       
    K 均值算法的一些缺点:
    - 受初值和离群点的影响每次的结果不稳定
    - 结果通常不是全局最优解而是局部最优解
    - 无法很好地解决数据簇分布差别比较大的情况
    - 不太适用于离散分类
    - 需要人工预先确定 K 值,该值和真实的数据分布未必吻合
    - K 均值只能收敛到局部最优,效果受初始值影响很大
    - 易受噪点的影响
    
    K 均值算法的一些优点:
    - 对大数据集,K 均值聚类算法相对是可伸缩和高效的
    - 计算复杂度是 O(NK_t)接近于线性,其中 N 是数据对象的数目,K 是聚类的簇,t 是迭代的轮数
    - 尽管算法经常以局部最优结束,但一般情况下达到的局部最优已经可以满足聚类的需求了
    
    K 均值算法的调优一般可以从以下角度出发:
    - 数据归一化和离群点处理
    - 合理选择 K 值
    - 采用核函数
    
    改进算法
    - K-means++
    - ISODATA
*** 高斯混合模型
    高斯混合模型(GMM)与 K-Means 算法类似,同样适用了 EM 算法进行迭代计算
    高斯混合模型假设每个簇的数据都是符合高斯分布(又叫正太分布),当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果
    多个高斯分布函数的线性组合对数据分布进行拟合。
    高斯混合模型的核心思想: 假设数据可以看作从多个高斯分布中生成出来的。在该假设下,每个单独的分模型都是标准的高斯模型
    高斯混合模型是一个生成式模型
    高斯混合模型的计算,便成了最佳的均值、方差、权重的寻找,这类问题通常通过最大似然估计来求解。
    EM 算法框架来求解该优化问题:
    EM 算法是在最大化目标函数时,先固定一个变量使整体函数变为凸优化函数,求导得到最值。利用好最优参数更新被固定的变量,循环。
    EM 的迭代过程,重复,直到收敛:
    - E 步骤: 根据当前的参数,计算每个点由某个分模型生成的概率
    - M 步骤: 使用 E 步骤估计出的概率,来改进每个分模型的均值,反差,权重
    高斯混合模型与 K 均值算法的相同点:
    - 都需要指定 K 值
    - 都使用 EM 算法求解
    - 往往只能收敛于局部最优
    - 相比于 K 均值的有点:可以给出一个属于某类概率是多少,不仅聚类,还可以计算概率密度估计,还可以生成新的样本点
*** 自组织映射神经网络(self-Organizing Map,SOM)
    - 无监督学习方法中的一类重要方法
    - 可以用作聚类、高维可视化、数据压缩、特征提取等多种用途
    - 融入了大量人脑神经元的信号处理机制,有着独特的结构特点
    - 也被称 Kohonen 网络
    - 具有保序映射的特点,可以将任意维度输入模式在输出层映射为一维或者二维图像,并保持拓扑结构不变
    - 神经元近邻者相互激励,远邻者相互抑制
    工作原理:
    - 本质上是一个两层的神经网络:输入层和输出层(竞争层)
    - 输出层节点是有拓扑关系的,根据需要(一维,二维,三维,多维)
    字组织的学习过程:
    - 初始化
    - 竞争
    - 合作
    - 适应
    - 迭代
    与 K-Means 是区别:
    - K-Means 需要预设 K,其结果受 K 影响较大,自组织映射神经网络不用
    - K-Means 为每个输入数据找到一个最相似的类后,只更新这个类的参数,自组织映射神经网络会更新邻近的节点
    - 相比而言,自组织映射神经网络的可视化比较好,而且具有优雅的拓扑关系图
    设计自组织神经网络网络和训练步骤:
    - 设定输出层神经元的数量
    - 设计输出层节点的排列
    - 初始化权值
    - 设计拓扑领域
    - 设计学习率
*** 聚类算法的评估
    为了评估不同聚类算法的性能优劣,需要了解常用的数据簇:
    - 以中心定义的数据簇: 倾向于球形分布,通常中心被定义为质心,即此数据簇中所有点的平均值
    - 以密度定义的数据簇: 这类数据集合呈现和周围数据簇不明显不同的密度,或稠密或稀疏
    - 以连通定义的数据簇: 这类数据集合的数据点和数据点之间有连接关系,真个数据簇表现为图结构
    - 以概念定义的数据簇: 这类数据集合中的所有数据点具有某种共同性质
    聚类评估的任务是估计在数据集上进行聚类的可行性,以及聚类方法产生结果的质量,分为三个子任务:
    - 估计聚类趋势
    - 判定数据簇数
    - 测定聚类质量
    评判聚类算法好坏:
    - 轮廓系数
    - 均方根标准偏差
    - R 方
** 第六章 概率图模型
*** 概率图模型的联合分布
    概率图模型最为精彩的部分就是能够用简洁清晰的图示表达概率生成的关系。
    通过概率图还原其概率分布不仅是概率图模型最重要的功能,也是掌握概率图模型最重要的标准。
    通过贝叶斯网络和马尔可夫网络的概率图还原其联合概率分布。
*** 概率图表示
    朴素贝叶斯模型通过预测指定样本属于特定类别的概率来预测该样本的所属类别。
    熵就是不确定的度量,熵越大,不确定性也就越大。
    最大熵原理: 是概率模型学习的一个准则,指导思想是在满足约束条件的模型集合中选取熵最大的模型,即不确定性最大的模型。
    最大熵模型归结为学习最佳的参数 w,使得$P_{w}(y|x)$最大化。
    概率图模型的角度理解,我们可以看到$P_{w}(y|x)$的表达形式非常类似于势函数的马尔可夫网络,其中变量 x 和 y 构成了一个最大团
*** 生成式模型与判别式模型
    生成式模型和判别式模型的区别是机器学习领域非常重要的基础知识。
    生成式模型是对联合概率分布$P(X,Y,Z)$进行建模,在给定观测集合 X 的条件下,通过计算边缘分布来得到变量集合 Y 的推断:
    $P(Y|X)=\frac{P(X,Y)}{P(X)}$
    判别式模型是直接对条件概率分布$P(Y,Z|X)$进行建模,然后消掉无关变量 Z 就可以得到对变量集合 Y 的预测:
    $P(Y|X)=\sum_{Z}P(Y,Z|X)$
    
    常见的概率图模型:
    + 生成式模型(先对联合概率分布进行建模,然后再通过计算边缘分布得到变量的预测)
      - 朴素贝叶斯
      - 贝叶斯网络
      - pLSA
      - LDA
      - 隐马尔可夫模型
    + 判别式模型
      - 最大熵模型(对条件概率分布进行建模)
      - 条件随机场(对序列数据进行建模)
*** 马尔可夫模型
    马尔可夫过程是满足无后效性的随机过程
    马尔可夫过程: 假设一个随机的过程中,$t_{n}$时刻的状态$X_{n}$的条件分布,仅仅与其前一个状态$X_{n-1}$有关,即$P(X_n|X1,X2...X_{n-1})=P(X_n|X_{n-1})$
    马尔可夫链: 时间和状态的取值都是离散的马尔可夫过程
    马尔可夫模型: 是对含有未知参数(隐状态)的马尔可夫链进行建模的生成的模型
    所有状态对于观测者都是可见的,因此在马尔可夫模型仅仅包括状态间的转移概率
    隐状态: 当前输入
    隐状态取值空间: 所有输入
    观测状态的取值空间: 所有输出
    初始状态的概率分布: 初始化的概率
    隐状态间的转移概率: 从当前输入转移到下一个输入的概率
    隐状态到观测状态的输出概率: 每次拿到输入属于输出的某个值的概率
    观测状态的序列: 输入的顺序
    隐状态的序列: 每次拿到输入的顺序
    
    + 隐马尔可夫模型的三个基本问题
      - 概率计算问题: 已知模型的所有参数,计算观测序列 Y 出现的概率,可以使用前向和后向算法求解
      - 预测问题: 已知模型所有参数和观测序列 Y,计算最大可能的隐状态序列 X,可使用经典的动态规划算法--维特比算法来求解最可能的状态序列
      - 学习问题: 已知观测序列 Y,求解使得该观测序列概率最大的模型参数,包括隐状态序列、隐状态之间的转移概率分布以及从隐状态到观测状态的概率分布,使用 Baum-Welch 算法进行参数学习
    隐马尔可夫模型通常用来解决序列标注问题,因此也可以将分词问题转化为一个序列标注问题来进行建模
    中文句子中的每个字做以下标注: B 表示一个词开头的第一个字,E 表示一个词尾的最后一个字,M 表示一个词中间的字,S 表示一个单字词,则隐状态的取值空间为{B,E,M,S}
    同时对隐状态的转移概率可以给出一些先验知识,B 和 M 后面只能是 M 或者 E,S 和 E 只能是 B 或者 S。
    每个字就是模型中的观测状态,取值空间为语料中的所有中文字。
    完成建模之后,使用语料进行训练可以分为有监督训练和无监督训练
    有监督训练即对语料进行标注,相当于根据经验得到了语料的所有隐状态信息,然后就可以简单的计数法来对建模中的概率分布进行极大似然估计。
    无监督训练可以用上下文提到 Baum-Welch 算法,同时优化隐状态序列和模型对应的概率分布。
    
    标注偏置问题(Label Bias Problem)
    - 隐马尔可夫模型等用于解决序列标注问题的模型中,常常对标注进行了独立性假设。
    - 局部归一化的影响,隐状态会倾向于转移到哪些后续状态可能更少的状态上,以提高整体的后验概率
    
    隐马尔可夫模型建模时考虑了隐状态间的转移概率和隐状态到观测状态的输出概率。
    隐状态(标注)不仅和单个观测状态相关,还和观测序列的长度、上下文等相关信息。
    
    最大熵马尔可夫模型:
    - 在建模时,去除了隐马尔可夫模型中观测状态相互独立的假设,考虑了整个观测序列,因此获得了更强的表达能力
    
    隐马尔可夫模型是一种对隐状态序列和观测状态序列的联合概率$P(x,y)$进行建模的生成式模型
    最大熵马尔可夫模型是直接对标注的后验概率$P(y|x)$进行建模的判别式模型
    
    条件随机场:
    - 在最大熵马尔可夫模型的基础上,进行了全局归一化
    - 解决了局部归一化带来的标注偏置问题
*** 主题模型
    基于词袋模型或 N-gram 模型的文本表示有一个明显的缺陷,就是无法识别出两个不同的词或词组具有相同的主题
    主题模型是能够将词或词组映射到同一个维度上。
    主题模型是一种特殊的概率图模型
    + 如何判定两个词是否具有相同的主题模型:
      - 如果两个产生概率都是比较高的
      - 而另一些不太相关的词汇产生的概率则是比较低的
      - 假设有 K 个主题
      - 每一维度代表一个主题
      - 权重代表这边文章术语这个特定主题的概率
    主题模型所解决的事情就是从文本库中发现有代表性的主题,并且计算出每篇文章对应哪些主题
    常见的主题模型:
    - pLSA: 用一个生成模型来建模文章的生成过程
    - LDA: pLSA 的贝叶斯版
    确定 LDA 模型中的主题个数:
    - 主题的个数是一个预先指定的超参数
    - 随机切分数据集:(6:2:1)
    - 评估指标:困惑度(perplexity)
    冷启动问题分类:
    - 用户冷启动: 指一个之前没有行为或行为极少的新用户进行推荐
    - 物品冷启动: 一个新上市的商品寻找到具有潜在兴趣的用户
    - 系统冷启动: 指如何为一个新开发的网站设计个性化推荐系统
    解决冷启动问题的方法一般是基于内容的推荐
   
** 第七章 优化算法
   机器学习算法 = 模型表征 + 模型评估 + 优化算法
   不同的优化算法对应的模型表征和评估指标不尽相同
*** 有监督学习的损失函数
    机器学习算法的关键一环是模型评估,而损失函数定义了模型的评估指标。
    没有损失函数就无法求解模型参数。
    在有监督学习中,损失函数刻画了模型和训练样本的匹配程度。
    对于回归问题,最常用的损失函数时平方损失函数
    
    平方损失函数时光滑函数,能够用梯度下降法进行优化,当预测值距离真值越远时,平方损失函数的惩罚力度越大,因此它对异常点比较敏感。
    
    绝对损失函数相当于是在做中指回归,相比做均值回归的平方损失函数,绝对损失函数对异常点更鲁棒性一些
*** 机器学习中的优化问题
    大部分机器学习模型的参数估计问题都可以写成优化问题。
    机器学习模型不同,损失函数不同,对应的优化问题哥不想听。
    线性回归、逻辑回归、支撑向量机对应的优化问题就是凸优化问题。
    可通过计算目标函数的二阶 Hessian 矩阵来验证凸性。
    对于凸优化问题,所有的布局极小值就是全局极小值。
    
    主成分分析对应的优化问题是非凸优化问题。还有矩阵分解,深度神经网络都非凸。
    非凸优化问题被认为比较难求解,我们可以借助 SVD 得到主成分分析的全局极小值。
*** 经典优化算法
    无约束优化问题的优化方法
    + 直接法
      - 第一个条件: 凸函数
      - $\nabaL(\theta\*)=0$,有闭式解
      - 经典例子:岭回归
    + 迭代法
      - 一阶法: 对函数做一阶泰勒展开,也称梯度下降法
      - 二阶法: 对函数做二阶泰勒展开,也称牛顿法
*** 梯度验证
*** 随机梯度下降法
    经典的梯度下降法所有训练数据的平均损失来近似目标函数。
    经典的梯度下降法在每次对模型参数进行更新时,需要遍历所有的训练数据。
    为了解决该问题,随机梯度下降法用单个训练样本的损失来近似平均损失,即对模型参数进行一次更新,大大加快了收敛速度。
    
    为了降低随机梯度的方差,从而使得迭代算法更加稳定,也为了充分利用高度优化的矩阵运算操作,在实际应用中我们会同时处理若干训练数据，
    该方法被称为小批量梯度下降法。
    
    对于小批量梯度下降法的使用,有三点需要注意:
    - 如何选取参数 m?最优的 m 通常会不一样,需要通过调参选取。
    - 如何跳转 m 个训练数据
    - 如何选取学习速率
*** 随机梯度下降法的加速
    随机梯度下降法本质上采用迭代方法更新参数,每次迭代在当前位置的基础上,沿着某一方向迈一小步抵达下一位置,然后在下一位置重复上述步骤。
    + 随机梯度下降法缺陷的解决方法: 惯性保持和环境感知
      - MoMentum(动量): 刻画惯性的物理量是动量,在惯性作用下继续前行,从而有机会冲出这片平坦的陷阱。(收敛速度快,收敛曲线更稳定)
      - AdaGrad: 采用历史梯度平方和来衡量不同参数的梯度的稀疏性,取值越小越稀疏。(随着时间推移,学习速率越来越小,从而保证了算法的最终收敛)
      - Adam: 将惯性保持和环境感知两个优点集于一身。(first MoMentum 惯性保持 + second AdaGrad 环境感知)
    + 以上三种方法的变种:
      - Nesterov Accelerated Gradient: 扩展了动量方法,计算未来可能位置处的梯度而非当前位置梯度,这个提前量设计让算法有了对前方环境预判的能力
      - AdaDelta 和 RMSProp: 两个方法类似,是对 AdaGrad 方法的改进
      - AdaMax: 该方法是基于 Adam 方法的一个变种方法,对梯度平法的处理由指数衰减平均改为指数衰减求最大值。
      - Nadam: 该方法可以看成 Nesterov Accelerated Gradient 版的 Adam
*** L1 正则化与稀疏性
    为什么希望模型参数具有稀疏性?
    - 稀疏性,说白了就是模型的很多参数是 0。这相当于对模型进行一次特征选择,只留下一些比较重要的特征,提高模型的泛化能力,降低过拟合的可能。
    L1 正则产生稀疏解的原因:
    - 角度一: 解空间形状,L2 正则项约束后的解空间是圆形,而 L1 正则项约束的解空间是多边形。多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。
    - 角度二: 函数叠加,在一些在线梯度下降法中,往往会采用截断梯度法来产生稀疏性这同 L1 正则项产生稀疏性的原理是类似的。
    - 角度三: 贝叶斯先验,L1 正则化相当于对模型参数引入了拉普拉斯先验,L2 正则化相当于引入了高斯先验,而拉普拉斯先验使参数为 0 的可能性更大

** 第八章 采样

** 第九章 前向神经网络
   
** 第十章 循环神经网络

** 第十一章 强化学习

** 第十二章 继承学习

** 第十三章 生成对抗网络
