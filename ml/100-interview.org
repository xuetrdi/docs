* 机器学习算法工程师的自我修养
  
** 第一章 特征工程
*** 特征归一化
    为什么需要对数值类型的特征做归一化?
    - 为了消除数据特征之间的量纲影响,我们需要对特征进行归一化处理,使得不同指标之间具有可比性
    最常用的两种归一化方法:
    - 线性函数归一化: 对原始数据进行线性变换,使结果映射到[0,1]的范围,实现对原始数据的等比缩放。
    - 零均归一化: 将原始数据映射到均值为 0,标准差为 1 的分布上。
    
    线性函数归一化:
               $X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$
    零均值归一化:
               $z=\frac{x-\mu}{\sigma}$
    
*** 类别型特征
    类别型特征(Categoridal Feature)主要是指性别，血型等在有限选项内取值的特征。
    类别型特征原始输入通常是字符串形式，除了决策树等少量模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，必须转换成数值类型才能正确工作
    
    在数据进行预处理时，应该怎样处理类别型特征？
    - 序列编码(Ordinal Encoding):通常用于处理类别间具有大小关系的数据。比如成绩
    - 独热编码(One-hot Encoding):通常用于处理类别间不具有大小关系的特征。
    - 二进制编码(Binary Encoding):二进制编码主要分两步，先用序号编码给每个类别赋予一个类别 ID,然后将类别 ID 对应的二进制编码作为结果。
*** 高维组合特征
    + 什么是组合特征?
      - 为了提高复杂关系的拟合能力,在特征工程中经常会把一阶离散特征两两组合,构成高阶组合特征。
    + 如何处理高维组合特征?
      - 把两个特征组成一个特征对
*** 组合特征
    + 怎样有效地找到组合特征?
*** 文本表示模型
    - 词袋模型(Bag of Words)
    - TF-IDF(Term Frequency-Inverse Document Frequency)
    - 主题模型(Topic Model)
    - 词嵌入模型(Word Embedding)
    文本表示模型以及优缺点:
    + 词袋模型
      - 最基础的文本表示模型是词袋模型:
      - 将每篇文章看成一袋子词,并忽略每个词出现的顺序
      - 以词切分,每篇文章表示成一个长向量,向量中的每一维代表一个词。
      - 该维度对应的权重反映了这个词在原文中的重要程度,常用 TF-IDF 计算权重: TF-IDF(t,d)=TF(t,d)xIDF(t),其中 TF(t,d)为单词在 t 在文档 d 中出现的频率,IDF(t)是逆文档频率。
        IDF(t)=log(文章总数/包含单词 t 的文章总数+1)
    + N-gram 模型
      - 连续出现的 n 个词组成的词组(N-gram)也作为一个单独的特征放到向量表示中去,构成 N-gram 模型。
      - 同一个词可能有多种词性变化,却具有相似含义。
      - 实际应用中一般会对单词进行词干抽取(Word Stemming)处理,即将不同词性的单词统一成为同一词干的形式。
    + 主题模型
      - 用于从文本库中发现有代表性的主题(得到每个主题上面词的分布特征),并且能够计算每篇文章的主题分布。
    + 词嵌入与深度学习模型
      - 词嵌入是一类将词向量化的模型的统称,核心思想是将每个词都映射成低纬空间 K 维(5-300)上一个稠密向量(Dense Vector),K 维空间的每一个维可以看作一个隐含的主题,只不过不像主题模型中那么直观。
*** Word2Vec
    谷歌 2013 年提出的 Word2Vec 是目前最常用的词嵌入模型之一。
    Word2Vec 实际是一种浅层的神经网络模型,它有两种网络结构,分别是 CBOW(Continues Bag of Words)和 Skip-gram。
    + Word2Vec 是如何工作的?
      - CBOW 的目标是根据上下文出现的词语来预测当前词的生成概率
      - Skip-gram 是根据当前词来预测上下文中各词的生成概率
      - CBOW 和 Skip-gram 都可以表示成由输入层(Input),映射层(Projection)和输出层(Output)组成的神经网络
    
    其中 w(t)是当前所关注的词,w(t-2),w(t-1),w(t+1),w(t+2)是上下文出现的词,这里的前后滑动窗口大小均设为 2。
    + CBOW 和 Skip-gram 的模型结构:
      - 输入层: 每个词由独热编码表示,即所有词均表示成一个 N 维向量,其中 N 为词汇表中单词的总数。
      - 映射层: K 个隐含单元的取值可以由 N 维输入向量以及连接输入和隐含单元之间 NxK 维权重矩阵计算得到,在 CBOW 中,还需要将各个输入词所计算出的隐含单元求和。
      - 输出层: 输出层向量的值通过隐含层向量(K 维),以及连接隐含层和输出层之间的 KxN 维权重矩阵计算得到。输出层也是 N 维向量。
      - Softmax: 计算每个词的生成概率。
    + Word2Vec 与 LDA(因狄利克雷模型)的区别和联系
      - LDA 是利用文档中单词的共现关系来对单词按主题聚类,也可以理解为对"文档-单词"矩阵进行分解,得到"文档-主题"和"主题-单词"两个概率分布。
      - Word2Vec 其实是对"上下文-单词"矩阵进行学习,其中上下文由周围的几个单词组成,由此得到的词向量表示更多地融入了上下文共现的特征.
      - 如果两个单词所对应的 Word2Vec 向量相似度较高,那么它们很可能经常在同样的上下文出现。
      - 主题模型和词嵌入两类的主要差异:主题模型通过一定的结构调整可以基于"上下文-单词"矩阵进行主题推理;词嵌入根据"上下文-单词"矩阵学习出现的隐含向量表示。
      - 主题模型和词嵌入两类方法最大的不同在于模型本身,主题模型是一种基于概率图模型的生成式模型,其似然函数可以写成若干条件概率连成的形式,其中也需要推测的隐含变量(即主题)
        而词嵌入模型一般表达为神经网络形式,似然函数定义在网络的输出上,需要通过学习网络的权重以得到单词的稠密向量表示。
*** 图像不足时的处理方法
    机器学习应用中经常会遇到训练数据不足的问题。
    处理方法:
    - 迁移学习
    - 生成对抗网络
    - 图像处理
    - 上采样技术
    - 数据扩充
   一个模型所能提供的信息一般来源于两个方面: 
   - 训练数据中蕴含的信息
   - 在模型的形成过程中(构造,学习,推理),人们提供的先验信息
   当数据不足时,说明模型从原始数据中获取的信息比较少,这种情况下想要保证模型的效果,就需要更多先验信息。
   
   先验信息可以作用在模型上
   - 让模型采用特定的内在结构、条件假设或添加其它一些约束条件
   - 先验信息也可以直接施加在数据集上,即根据特定的先验假设去调整、变换、扩展训练数据,让其展现出更多更有用的信息,以利于后续模型的训练和学习。
   
   在图像分类任务上,训练数据不足带来的问题主要表现在**过拟合**方面,在训练集上效果很好,在测试集上表现很差。
   处理过拟合的方法主要两类:
   - 基于模型的方法: 简化模型(非线性简化为线性),添加约束项(L1/L2 正则),集成学习,Dropout
   - 基于数据的方法: 对图像进行扩充增强,随机旋转,平移,缩放,裁剪,填充,翻转;添加噪声(椒盐噪声,高斯白噪声);颜色变换;亮度,清晰度,对比度,锐度;GAN
   借助已有的其它模型或数据来进行迁移学习:
   - 在大规模数据集上预训练好的通用模型,在针对目标任务的小数据集上进行微调,看作一种简单的迁移学习。
   
** 第二章 模型评估
*** 评估指标的局限性
    没有测量,就没有科学。
    模型评估主要分为离线评估和在线评估两个阶段。
    针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。
    针对性选择合适的评估指标、根据评估指标的反馈进行模型调整。
    + 各种指标
      - 准确率: Accuracy
      - 精确率: Precision
      - 召回率: Recall
      - 均方根误差: Root Mean Square Error(RMSE)
    
    + 准确率的局限性
      - Accuracy = 正确分类的样本量/总样本量
      - 缺陷性: 当负样本占 99%,分类负样本也可以获得 99%的准确率。所以不均衡是准确率影响的最主要因素。
      - 解决方法: 可以使用更为有效的平均准确率(每个类别下的样本准确率的算术平均)作为模型评估的指标。
    
    + 精确率与召回率的权衡
      - 精确率是指分类正确的正样本个数占分类器判定为正样本的样本个数比例: TP/(TP+FP+FN)
      - 召回率是指分类正确的正样本占真正的正样本个数的比例: TP/(TP+FP)
      - P-R 曲线能更好的衡量模型的性能
    
    + F1 score 和 ROC 曲线也能综合地反映一个排序模型的性能。
      - F1 score 是精准率和召回率的调和平均值。

    + RMSE 经常被用来衡量回归模型的好坏
      - RMSE 能够很好地反映地反映回归模型预测值与真实值的偏离程度。
      - 如果存在个别偏离程度非常大的离群点时,即使数量非常少,也会让 RMSE 指标变得很差。
      - 存在比 RMSE 的鲁棒性更好的指标,比如绝对百分比误差(Mean Absolute Percent Error，MAPE)
*** ROC 曲线
    ROC 曲线：Receiver Operating Characteristic Curve(受试者工作特征曲线),
    + ROC 曲线
      - 横坐标为假阳性率(False Positive Rate,FPR): FPR = FP/N, N 是真实的负样本的数量(N=TN+FN)
      - 纵坐标为真阳性率(True Positive Rate, TPR): TPR = TP/P, P 是真实的正样本的数量(P=TP+FP)
    + 绘制 ROC 曲线
      - 统计正样本和负样本的数量 P 和 N
      - 把横轴的刻度间隔设置为 1/N,纵轴的刻度间隔设置为 1/P
      - 根据模型输出的预测概率对样本进行排序(从高到低)
      - 依次遍历样本,同时从零开始绘制 ROC 曲线,没遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线,每遇到一个负样本就沿横轴方向绘制一个刻度间隔的曲线,直到遍历所有样本
      - 曲线最终停在(1,1)这个点,整个 ROC 曲线绘制完成。
    + 如何计算 AUC
      - AUC 指的是 ROC 曲线下的面积大小
      - 该值能够量化地反映基于 ROC 曲线衡量出的模型性能
      - 计算 AUC 值只需要沿着 ROC 横轴做积分就可以了
      - ROC 曲线一般都处于 y=x 这条直线的上方,如果不是,只要把模型预测概率反转成 1-p 就可以得到一个更好的分类器
      - AUC 的取值一般在 0.5-1 之间,AUC 越大,说明分类器越可能把真正的正样本排在前面,分类性能越好
    + ROC 曲线和 P-R 曲线的特点
      - P-R 曲线经常被用来做分类和排序模型,当正负样本发生变化时,一般会发生剧烈的变化
      - ROC 曲线有一个特点,当正负样本的分布发生变化时,ROC 曲线的形状能够基本保持不变,让 ROC 曲线能够降低不同测试集带来的干扰,更加客观地衡量模型本身的性能。
      - 在实际问题数量往往很不均衡,P-R 曲线的变化就会非常大,ROC 曲线则更加稳定地反映模型本身的好坏。
*** 余弦距离的应用
    评估样本间距离
    在机器学习中特征表示成向量,分析两个特征向量之间的相似性常用余弦相似度来表示。
    $$\cos(A,B)=\frac{A\cdot B}{||A||_{2}||B||_{2}}$$  : 即两个向量夹角的余弦,关注的是向量之间的角度关系,并不关心它们的绝对值的大小,其取值范围是[-1,1]
    在 Word2Vec 中,其向量的模长是经过归一化的,此时欧式距离与余弦距离有着单调的关系
    $$||A-B||_{2}=\sqrt{2(1-\cos(A,B))}$$
    其中欧氏距离: $||A-B||_{2}$
    其中余弦相似度: $\cos(A,B)$
    表示余弦距离: $(1-cos(A,B))$
    总体来说,欧式距离体现**数值**上的绝对差异,而余弦**距离**体现方向上的相对差异。
    
    三条距离公理:正定性、对称性、三角不等式成立,则该实数可称为这对元素之间的距离。
    余弦距离满足正定性和对称性,但不满足三角不等式,因此它并不是严格定义的距离。
    
*** A/B 测试的陷阱
    在离线评估后,为什么还要进行 A/B 测试
    - 离线评估无法完全消除模型过拟合的影响,因此得出的离线评估结果无法完全替代线上评估结果。
    - 离线评估无法完全还原线上的环境。
    - 线上系统的某些商业指标在离线评估中无法计算。离线测试只针对模型本身进行评估,与模型无关的其它指标,特别是商业指标。
    A/B 测试的主要手段是进行用户分桶,即将用户分成实验组和对照组,对实验组的用户使用新模型,对对照组的用户使用旧模型。
    分桶过程中要注意:无偏性,和随机性。
*** 模型评估的方法
    模型验证中主要的方法：
    - Holdout 检验: 简单、直接,将样本按照比例划分,计算 ROC 曲线,精确率,召回率等指标评估模型性能。缺点:分组不平衡。
    - k-fold 交叉检验: 全部样本划分成 k 个大小相等的样本子集。依次把当前子集作为验证集,其余所有子集作为训练集,进行模型的训练和评估。最后把 k 次评估指标的平均值作为最终的评估指标。每次留下一个作为验证集,其余作为测试集。
    - 自助法: 不管是 Holdout 检验还是交叉检验,都是基于划分训练集和测试集的方法进行模型评估的。基于自助采样法的检验方法。
*** 超参数调优
    超参数调优方法:
    - 网格搜索: 通过查找搜索范围内的所有的点来确定最优值,通过网格搜索很大概率能找到全局最优值,目标函数一般非凸,所以可能错过全局最优值。
    - 随机搜索: 随机搜索的思想与网格搜索比较相似,只是不再测试上界和下界之间的所有值,而是随机范围中随机选取样本点。
    - 贝叶斯优化: 贝叶斯优化算法是通过目标函数形状进行学习,找到使目标函数向全局最优值提升的参数。一旦找到局部最优解,会在该区域不断采样,所以很容易陷入局部最优值。
    超参数搜索算法一般包含以下因素
    - 目标函数
    - 搜索范围
    - 其它参数,搜索步长
*** 过拟合与欠拟合
    过拟合: 在训练集上拟合很好,在测试集上表现很差。
    欠拟合: 没有很好地捕捉到数据的特征,不能够很好地拟合数据。
    降低过拟合风险的方法:
    - 增加训练数据
    - 降低模型复杂度
    - 正则化
    - 集成学习方法,把多个模型集成在一起,降低单一模型的过拟合风险
    降低欠拟合风险的方法:
    - 添加新特征
    - 增加模型复杂度
    - 减小正则化系数:正则化是用来防止过拟合,但当模型出现欠拟合想象,则需要有针对地减小正则化系数
** 第三章 经典算法
*** SVM 模型推导
    任意线性可分的两组点,它们在 SVM 分类超平面上的投影都是线性不可分的。
    + KKT 条件
    + SVM 的分类结果仅依赖于支持向量,也就是 SVM 有用极高运行效率的关键之一
    + 凸优化理论中的超平面分离定理(SHT)
      - 对于不相交的两个凸集,存在一个超平面,将两个凸集分离。
      - 对于二维的情况,两个凸集间距离最短两点连线的中垂线就是一个将它们分离的超平面
    借助这个定义,可以先对线性可分的两组点各自求各自的凸包,不难发现,SVM 求得的超平面就是两个凸包上距离最短的两点连线的中垂线。
    根据凸包的性质很容易知道:
    - 凸包上的点要么是样本点,要么处于两个样本点的连线上
    两个凸包间距离最短的两个点可以分为三种情况：
    - 两边的点均为样本点
    - 一边的点为样本点,另一边的点在样本点的连线上
    - 两边的点均在连线上
    核函数
    SMO(Sequential Minimal Optimization)
*** 逻辑回归   
    逻辑回归处理的是分类问题,因变量取值是一个二元分布,模型得出的是期望: $E\left[y|x;\theta \right]$
    线性回归处理的是回归问题,线性回归求解的问题是: $y=\theta^{T}x+\epsilon$, $\epsilon$ 代表误差项
    逻辑回归的因变量是离散的,线性回归中的因变量是连续的,逻辑回归可以看做广义线性模型,在因变量 y 服从二元分布时的一个特殊情况，
       而使用最小二乘法求解线性回归时,我们认为因变量 y 服从正太分布
    逻辑回归和线性回归二者都使用了极大似然估计来对训练样本进行建模
    多分类问题的概率服从于几何分布,使用多项逻辑回归(Softmax Regression)来进行分类
    多项逻辑回归实际上是二分类逻辑回归在多标签分类下的一种拓展;当存在样本可能属于多个标签时,可以训练 k 个二分类的逻辑回归分类器
*** 决策树
    决策树是一种自上而下,对样本数据进行树形分类的过程,由节点和有向边组成
    决策树作为最基础、最常见的有监督学习模型,常被用于分类问题和回归问题
    决策树的三个处理过程:
    - 特征选择
    - 树的构造
    - 树的剪枝
    根据不同的特征和属性,建立一颗树形的分类结构。
    若干不同的决策树中选取最优的决策树是一个 NP 完全问题,通常采用启发式学习的方法构建一颗决策树。
    决策树的常用算法:
    - ID3: 最大信息增益
    - C4.5: 最大信息增益比
    - CART: 最大基尼指数(Gini),Gini 是描述的数据纯度
    决策树的剪枝方法(防止过拟合):
    - 预剪枝
    - 后剪枝
** 第四章 降维
   为什么要降维?
   - 对原始数据进行特征提取,有时会得到比较高的特征向量,计算的速度特别慢
   - 通过降维来寻找数据内部的特征,从而提升表达能力,降低训练复杂度
   常见的降维的方法有:
   - 主成分分析
   - 线性判别分析
   - 等距映射
   - 局部线性嵌入
   - 拉普拉斯特征映射
   - 局部保留映射
   PCA:
   - 线性,**非监督**,全局降维算法
   - 旨在找到数据的主成分,并利用这些成分表征原始数据,从而达到降维的目的
   - 通过坐标系旋转使得新坐标系能表征原生数据,完成数据的降维
   - 使得原生数据更分散,则信号具有较大方差,噪声具有较小方差(信噪比)
   PCA 的目标:
   - 信噪比越小意味着数据的质量越差,即最大化投影方差,也就是让数据在主轴投影的方差最大
   - 投影后的方差就是协方差矩阵的特征值
   - 最大的方差也就是协方差矩阵最大的特征值
   - 最佳投影方向就是最大特征值所对应的特征向量
   - 次佳投影方向就是最佳投影方向的正交空间中,是第二大特征值所对应的特征向量
   PCA 求解方法:
   - 对样本数据进行中心化处理
   - 求样本协方差矩阵
   - 对协方差矩阵进行特征值分解,将特征值从大到小排列
   - 取特征值前 d 大对应的特征向量,通过以下映射将 n 维样本映射到 d 维度
   PCA 的不足:
   - PCA 是一种线性降维方法,有一定局限性
   - 可以通过核映射对 PCA 进行扩展得到核主成分(KPCA)
   非线性降维(流形映射):
   - 等距降维
   - 局部线性嵌入
   - 拉普拉斯特征映射
  
   线性判别分析:
   - **有监督**学习算法
   - 经常被用来对数据进行降维
   中心思想:
   - 最大化类间距离和最小化类内距离
   - 使得类间距离尽可能大的投影方式,现在只需要同时优化类内方差,使其尽可能小。
   - LDA 已被证明是非常有效的一种降维方法
   - 线性模型对于噪声的鲁棒性比较好
   - 由于模型简单,表达能力有一定局限性,通过引入核函数扩展 LDA 方法以处理分布较为复杂的数据
   
   多个类别标签高维数据的 LDA 求解方法
   1. 计算数据集中每个类别样本的均值向量($\mu_{j}$)和总体均值向量($\mu$)
   2. 类内散度矩阵($S_{w}$),全局散度矩阵($S_{t}$),得到类间散度矩阵($S_{b}=S_{t}-S_{w}$)
   3. 对矩阵($S_{w}^{-1}S_{b}$进行特征值分解,将特征值从大到小排列
   4. 取特征值前 d 大对应的特征向量,则将 n 维降到 d 维
   PCA 与 LDA 的区别:
   - PCA 选择的是投影后数据方差最大的方向,主成分表示原始数据可以去除冗余的维度,达到降维
   - LDA 选择的是投影后类内方差小、类间方差大的方向
   特征脸(Eigenface):
   - 基于 PCA 的人脸识别方法也称为特征脸方法,该方法将人脸图像按行展开形成一个高维向量,对多个人脸特征的协方差矩阵做特征值分解,其中较大特征值对应的特征向量具有与人脸相似的形状,故称为特征脸
   基本准则:
   - 对无监督的任务使用 PCA 进行降维
   - 对有监督的则应用 LDA
   - PCA 和 LDA 这种经典的线性降维方法,对于非线性数据,可以通过核映射等方法对二者分别进行扩展以得到更好的降维效果。
** 第五章 非监督学习
*** K 均值聚类
    分类问题属于监督学习,聚类则是非监督学习。
    K 均值聚类(K-Means Clustering)是最基础和最常用的聚类算法。
    基本思想:通过迭代方式寻找 K 个簇(Cluster)的一种划分方案,使得聚类结果对应的代价函数最小。
    代价函数可以定义为各个样本距离所属簇中心点的误差平方和:
    $J(c,\mu)=\sum_{i=1}^{M}\|x_{i}-\mu_{c_{i}}\|^2$ , $x_{i}$代表第 i 个样本,$c_{i}$是$x_{i}$所属的簇,$\mu_{c_{i}}$对应的中心点,M 是样本总量
    
    EM(Expectation-Maximization:最大期望)算法
    
    K 均值算法的具体步骤:
    1. 数据预处理,如归一化,离群点处理
    2. 随机选取 K 个簇中心,记为$\mu_{1}\,\mu_{2}\,...\mu_{K}$
    3. 定义代价函数:
    4. 令 t=0,1,2,...为迭代步数,重复下面过程直到 J 收敛
       
    K 均值算法的一些缺点:
    - 受初值和离群点的影响每次的结果不稳定
    - 结果通常不是全局最优解而是局部最优解
    - 无法很好地解决数据簇分布差别比较大的情况
    - 不太适用于离散分类
    - 需要人工预先确定 K 值,该值和真实的数据分布未必吻合
    - K 均值只能收敛到局部最优,效果受初始值影响很大
    - 易受噪点的影响
    
    K 均值算法的一些优点:
    - 对大数据集,K 均值聚类算法相对是可伸缩和高效的
    - 计算复杂度是 O(NK_t)接近于线性,其中 N 是数据对象的数目,K 是聚类的簇,t 是迭代的轮数
    - 尽管算法经常以局部最优结束,但一般情况下达到的局部最优已经可以满足聚类的需求了
    
    K 均值算法的调优一般可以从以下角度出发:
    - 数据归一化和离群点处理
    - 合理选择 K 值
    - 采用核函数
    
    改进算法
    - K-means++
    - ISODATA
*** 高斯混合模型
    高斯混合模型(GMM)与 K-Means 算法类似,同样适用了 EM 算法进行迭代计算
    高斯混合模型假设每个簇的数据都是符合高斯分布(又叫正太分布),当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果
    多个高斯分布函数的线性组合对数据分布进行拟合。
    高斯混合模型的核心思想: 假设数据可以看作从多个高斯分布中生成出来的。在该假设下,每个单独的分模型都是标准的高斯模型
    高斯混合模型是一个生成式模型
    高斯混合模型的计算,便成了最佳的均值、方差、权重的寻找,这类问题通常通过最大似然估计来求解。
    EM 算法框架来求解该优化问题:
    EM 算法是在最大化目标函数时,先固定一个变量使整体函数变为凸优化函数,求导得到最值。利用好最优参数更新被固定的变量,循环。
    EM 的迭代过程,重复,直到收敛:
    - E 步骤: 根据当前的参数,计算每个点由某个分模型生成的概率
    - M 步骤: 使用 E 步骤估计出的概率,来改进每个分模型的均值,反差,权重
    高斯混合模型与 K 均值算法的相同点:
    - 都需要指定 K 值
    - 都使用 EM 算法求解
    - 往往只能收敛于局部最优
    - 相比于 K 均值的有点:可以给出一个属于某类概率是多少,不仅聚类,还可以计算概率密度估计,还可以生成新的样本点
** 第六章 概率图模型
   
** 第七章 优化算法

** 第八章 采样

** 第九章 前向神经网络
   
** 第十章 循环神经网络

** 第十一章 强化学习

** 第十二章 继承学习

** 第十三章 生成对抗网络
