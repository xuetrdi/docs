* PCA
  
** OverView
   两种针对数值型特征,更为通用的降维方法：
   - 主成分分析(PCA): Principal Component Analysis
   - 奇异值分解(SVD): Singular Value Decomposition)
   这两种方法是从矩阵分析的角度出发,找出数据分布之间的关系,从而达到降维的目的,因此并不需要监督式学习中样本标签和特征的关系
** PCA 分析法的主要步骤
   使用矩阵来表示数据集: m 个样本,n 维特征,每行一个样本,每一列表示一个特征。
   降维就是找到一种变换,可以降低这个矩阵的列数,特征维数,尽可能保留原始数据中有用的信息。
   1. 标准化样本矩阵中的原始数据
   2. 获取标准化数据的协方差矩阵
   3. 计算协方差矩阵的特征值和特征向量
   4. 依照特征值的大小,挑选主要的特征向量
   5. 生成新的特征
*** 标准化原始数据
    $x^{\prime}=\frac{x-\mu}{\sigma}$
    - x: 为原始值
    - μ: 为均值
    - σ: 为标准差
    - x': 为变换后的值
*** 获取协方差矩阵
    协方差(Covariance): 协方差是用于衡量两个变量的总体误差。
    两个变量: x,y
    采样数量都是: m
    则协方差的计算公式为:
    $$
    \operatorname{cov}(x, y)=\frac{\sum_{k=1}^{m}\left(x_{k}-\overline{x}\right)\left(y_{k}-\overline{y}\right)}{m-1}
    $$
    - 其中$x_k$表示变量 x 的第 k 个采样数据
    - $\overline{x}$: 表示 k 个采样的平均值,当两个变量相同时,协方差就变成了方差
    协方差矩阵: 列与列之间的协方差,协方差组成的矩阵
    $$
    \operatorname{cov}\left(X_{, i}, X_{, j}\right)=\frac{\sum_{k=1}^{m}\left(x_{k, i}-\overline{X_{i}}\right)\left(x_{k, j}-\overline{X_{j}}\right)}{m-1}
    $$
    - $X_{,i}$: 表示第 i 列的列向量
    - $X_{,j}$: 表示第 j 列的列向量
    - $\overline{X_{,i}}$: 表示第 i 列的平均值
    - $\overline{X_{,j}}$: 表示第 j 列的平均值
    协方差矩阵为:
    第一列和第一列协方差 第一列与第二列协方差 ...  第一列与第 n 列协方差
    第二列与第一列协方差 第二列与第二列协方差 ...  第二列与第 n 列协方差
      ...               ...                   ...
    第 n 列与第一列协方差 第 n 列与第二列协方差 ...  第 n 列与第 n 列协方差
    
    由此可以看出:协方差矩阵是一个对称矩阵,对称矩阵的主对角线上的值就是各维度特征的方差
*** 计算协方差矩阵的特征值和特征向量
    矩阵中的特征值和特征向量与机器学习中的不同,是线性代数中两个非常重要的概念。
    
    向量 v 左乘一个矩阵 X 看作对 v 进行旋转和拉伸,而这种旋转和拉伸都是由于左乘矩阵 X 后,所产生的运动所导致的。
    - 特征向量 v 表示了矩阵 X 运动的方向
    - 特征值λ表示了运动的幅度
    - 两者结合就能表示左乘矩阵 X 带来的效果,因此被看作是矩阵的特征
    - 在 PCA 中就是特征向量,而对应的特征值的大小,就是表示这个特征向量或者说主成分的重要程度。
    - 特征值越大,重要程度越高,我们优先这个主成分,并利用这个主成分对原始数据进行变换
*** 挑选主要的特征向量,转换原始数据
    按照所对应的λ数值的大小,对这 k 组的 v 排序,排名靠前的 v 就是最重要的特征向量。
    特征值最大的值对应的特征向量就是主成分。
